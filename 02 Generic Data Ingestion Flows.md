# Generic Data Ingestion Flows

Let us understand how to build Generic Data Ingestion flows in this session. 

* Pre-requisites
* Recap of simple pipeline
* Recap of Flowfiles and Attributes
* Overview of Scheduling
* Files to Flowfiles - Options
* Copying Flowfiles to Files
* Deleting Files
* Build Generic Pipeline
* Quick Overview of NiFi Expression Language

The content will be free, however if you want to watch videos to understand key concepts feel free to [join our YouTube Channel as a member](https://www.youtube.com/channel/UCakdSIPsJqiOLqylgoYmwQg/join).

[![NiFi - Building Generic Data Flows](http://img.youtube.com/vi/iZQI-i8PWY4/0.jpg)](http://www.youtube.com/watch?v=iZQI-i8PWY4 "NiFi - Building Generic Data Flows")

## Pre-requisites
Here are the pre-requisites before we get into the session. We will be using single node development server to explore NiFi in-depth.
* We are supposed to have an environment where Hadoop and NiFi are setup.
* Run `jps` to ensure whether HDFS Components and NiFi are running or not.
* In case if HDFS and NiFi processes are not running, run following commands to get the environment ready.
```
start-dfs.sh
start-yarn.sh
nifi.sh start
# You can run jps and see if all the Java processes related to HDFS and NiFi are running or not.
```
* We also need to have **retail_db** data setup under **/data** folder. Run `ls -ltr /data/retail_db` to see if we have the data we are looking for or not.
* If data is missing make sure to setup the data from this [GitHub Repository](https://github.com/dgadiraju/retail_db).
* For the OS user we are using we need to have user space under HDFS. You can validate by running `hdfs dfs -ls /user/$USER`

## Recap of simple pipeline
Earlier we have understood simple data ingestion pipeline using NiFi.
* Processor Group
* Processors
* Settings
* Scheduling
* Properties
* Connecting multiple Processors

## Recap of Flowfiles and Attributes
Let us recap of core data structures as part of NiFi.
* Flowfiles are the ones which will contain the data.
* Attributes consists of metadata generated by processors.
* We can review the contents of flowfiles using Data Provenance of the processor.
* We typically don't have to manipulate flowfiles related to our data as part of the ingestion process. However NiFi provide processors to manipulate the data in flowfiles as well.
* In this session we will ingest the data with out applying any transformations.

## Overview of Scheduling
NiFi supports 2 types of scheduling.
* Timer based
* Cron based

## Files to Flowfiles - Options
Here are the design patterns with respect to gettig data from files to flowfiles.
* List + Fetch (ListFile + FetchFile)
* Get (GetFile)
* Processors which start with **List** (for example ListFile) does not take any input and hence it can be first processor in our ingestion pipeline to copy files.
* **List** processors typically read metadata of the files and NiFi will maintain the state to get the delta files (files that are added since last trigger).
* We need to use corresponding **Fetch** processor to fetch the files using the path and file name from upstream **List** Processors.
* We can directly use **Get** processor to get the files. Typically they also does not take input.
* However, unlike **List** processors, **Get** processors does not maintain state. We need to take care of capturing the delta.
We can get data from local file system, HDFS, AWS S3, Azure Blob, SFTP and many more. All of them provide both the options (List + Fetch or Get).

Flowfiles can also be saved to Files in all supported file systems.

## Copying Flowfiles to Files
We can copy files to target file systems using **Put** Processors.
* **PutHDFS** can be used to save files to HDFS.
* **PutFile** can be used to save files to local file system.
* We can also save files to cloud based file systems such as **S3**, **Azure Blob** etc.
* We will also have options such as compression to compress files as they are placed into target file system.
* For processors like **PutHDFS** we also have replacement strategy (fail, replace, ignore, append etc).

## Deleting Files
We can also delete files from any standard file system using **Delete** Processors.
* **DeleteHDFS** can be used to delete files from HDFS.
* Similar processors are available to delete files from other file systems.
* We need to have write permissions to delete files from underlying file systems.

## Build Generic Pipeline

Here are the instructions building generic pipeline.
* Use **ListFile** to get the files recursively.
* Configure Age Attributes as per requirement.
* Use **FetchFile** to fetch files.
* Update attribute to define target location using Update Attribute Processor. We can use NiFi Expression Language for the same.
```
${absolute.path:substringBeforeLast('/'):substringAfterLast('/')}
```
* Use **PutHDFS** to save the files in the HDFS. We can simulate source directory structure while writing data to target file system.

## Quick Overview of NiFi Expression Language

Let us understand more about NiFi Expression Language.
* It is extensively used to mutate flowfiles or attributes.
* We can also use NiFi Expression Language as part of conditional flows using processors such as **Route On Attribute**.
* NiFi Expression Language provides robust set of functions for data manipulation as well as traversing to JSON Paths.
* We have used expression language as part of previous topics in this module. We will see more as we get into more complex modules.
